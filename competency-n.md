---
biblio-files: '/home/jason/docs/school/references'
bibliography: '/home/jason/docs/school/references.bib'
competency: |
    Each graduate of the Master of Library and Information Science program
    is able to evaluate programs and services based on measurable criteria.
csl: 'apa.csl'
title: Competency N
...

Meaning and Articulation of Competency
--------------------------------------

All programs and services offered by library ans information
organizations exist to benefit their users, and any programs that do not
serve their users should be changed or ended. In an environment of
decreasing budgets and increasing needs, many programs and services are
competing with one another for scarce resources. It is therefor vital
for all such services to be judged fairly on their effectiveness and
worth. The most effective way of evaluating services fairly is by
choosing set specific goals or outcomes, and clear data that can be used
to measure how well those goals are being met. The most important aspect
here is that the criteria by which services are judged are quantifiable,
with defined possible measurements to determine if it meets the declared
goals or not. Without clear and specific criteria, any judgement is
largely just guessing, and it becomes impossible to determine which
services are effective and which are not.

Preparation and Evidence
------------------------

I have been a retail supervisor for several years, with ever increasing
responsibility, and as such I have firsthand experience in both judging
others' performance and having my own performance judged by specific
criteria. In retail, this is often very cut and dried, with easily
compared numbers such as sales versus goal or versus the previous year,
quotas filled (or not) of upsell items, and scores on customer
satisfaction surveys. Unfortunately, I have also been able to see the
consequences of unclear goals and unmeasurable criteria, as my company
recently (and briefly) had a CEO that had vague and lofty goals like
being people's "favorite" store, with no way of measuring success, and
as a result the company has found itself in much worse condition than
before this CEO became involved with it. His actions have shown clearly
that measurable and realistic goals are necessary for the success of a
project.

Of course, the goals and metrics of a LIS organization are usually very
different from those of a business. Collins (2005) points out that
social services cannot be measured with the same yardstick as a
business, and that it is often difficult to measure the success of a
social service in easy to understand numbers. Nonetheless, measurement
is possible, and necessary, using whatever evidence can be found, though
this evidence is usually less direct than for a business. With this in
mind, I have chosen three pieces of evidence to show what I have learned
in this competency. The first is an evaluation of online reference
services from local libraries. The second is a proposal for a program in
a public library that partners with a local teen club. The third is a
report comparing and critiquing two library children's departments.

### First Piece of Evidence: Online Reference Evaluation, LIBR 210

<aside>

[Report on Reference Services]({{ site.url }}/evidence/libr210/reference-report/)

</aside>

One of my earliest attempts to apply measurable goals, within my MLIS
course of study, was during my beginning Reference Services class in the
Spring of 2012. In one project for this class, I had to evaluate the
effectiveness of two Online Reference resources, both comparing them
with each other and against an ideal for them to reach for. In this
critique, I tested the "Ask a Librarian" forms from the Ramsey County
and Hennepin County library systems, both within the same metro area. I
used the same question, asked with the same wording, for both, so that
the comparison was fair. I measured both the forms as presented on their
websites, and the answers I received for my question, against the
criteria for a successful online reference interaction described in
Ross, Nilsen, & Radford (2009).

One strike against both services was that the form for entering
questions was little more than a large text box, with no suggestions or
questions to help narrow down the reference question before it got to
the librarian. In the responses themselves, both missed several
opportunities, though the librarian from Hennepin County did better.
Ross et al. (2009) recommend restating the question to be sure it is
understood, and asking follow-up questions themselves if it is unclear.
Neither librarian asked for further clarification, even though it was
obvious that the Ramsey County librarian did not fully understand the
original question. The authors suggest a combination of including
resources, when sure about what the question is asking, and library
instruction so that the user can conduct similar searches themselves.
The Hennepin County library did include some detailed library
instruction, both for her own library's catalog and for outside
resources, while the Ramsey County librarian only gave a brief
suggestion for a catalog search. Neither librarian actually suggested
any resources in response to the question asked. While the Hennepin
County librarian conducted a better and more thorough online reference
interaction, neither successfully responded going by the given criteria.

The information gathered from interactions like this could easily be
applied in order to help improve the online reference services of both
libraries. The book cited gives clear best practices for such
interactions. By comparing responses given to the criteria listed, one
could measure how well responses on average follow the guidelines, for
example taking on average of how many responses asked follow-up
questions, how many restated the question for understanding, and how
many attempted to answer the question with possible resources. This data
would then be used to improve the service by, for example, training
librarians on the criteria that they consistently miss, or altering the
common response script (if there is one) to better prompt for clarifying
questions. In this assignment I was able to measure failure of criteria
myself, and going forward I can see how to apply that same criteria in
my own service.

### Second Piece of Evidence: Teen Program Elements Chart, LIBR 261A

<aside>

[Program Design Elements]({{ site.url }}/evidence/libr261a/program-design/)

</aside>

In the fall of 2012 I took a course on programs and services for young
adults, and this course was very clear on the need for measurable and
demonstrable outcomes for any programs put into practice. Programs are
not created in a vacuum, and should be designed in such a way that they
benefit the specific population for which they are intended. Ideally,
this means that they are created with the input and partnership of the
intended audience, or even their full leadership, with the library
serving as the lesser partner. This can only work, though, if it is made
clear at the outset not only what the program is, but what it is
intended to accomplish to benefit the community, and how that
accomplishment will be measured. In support of this ideal, I created for
one assignment a project proposal for a partnership with a local teen
community, which included measurements for success.

In developing the goals and measurement criteria for this project, I
took my cue from Collins (2005) by having high-level goals for the
benefit of the community, along with what evidence can be used to
measure the success of those goals. The project is intended to benefit
both the library itself, by building relationships with the community,
and the teens who are to participate in the project, by supporting
several developmental assets (Search Institute, 2007). While these goal
as described are difficult to measure, in the project proposal I also
define related criteria that *can* be measured, and used as evidence
that the goals are successful. For example, library use of equipment
related to the project can be measured, to see if circulation has
changed in response, attendees of the project can be surveyed to
discover if the cross-institution marketing was what brought them in,
and the participants in the project can be surveyed after the event to
see if their club membership has improved as a result of the project.

Just as important as measuring the success (or lack thereof) of any
project or service is the response taken as a result of those
measurements. It is not enough to say whether something worked or not,
but rather it is best to also define what the consequences should be for
success or failure. At the simplest level, a program can either be
continued or ended. Often, though, a service has more complex outcomes
than simply success or failure, and the design of the service or program
should include plans for what to do next using the results of whatever
criteria are measured. For this project, I included specific actions to
be taken after the program has ended, including changes to the library's
collection and equipment if usage increases from the program,
development of the library's collection from the products of the
program, and plans to hold further, similar programs if this one is a
success. After all, evaluation of a program is for the purpose of
improving service, not ending it.

With this project, I learned how difficult it can be to find measurable
criteria to judge a service or program, though it remains just as
necessary as in any business or organization. I am used to measuring
outcomes based on simple and clear numbers, such as sales or customer
satisfaction surveys. While similar tools can be used in LIS settings,
they are limited in their applicability, and other less obvious criteria
must be found. Goals within LIS service are often somewhat abstract, and
almost impossible to measure directly, but because resources are
competitive and failed programs are wastes of time, some measure of
success must be used, usually something that is an indirect result of
the service itself. The trick, then, is to be able to see the
connections between the abstract, ideal goals, and the concrete,
measurable results. I have been able to make that connection with this
project, and have a much better grasp on how to do so in the future.

### Third Piece of Evidence: Comparing Children's Departments, LIBR 232

<aside>

[Library Visit - Children's Area]({{ site.url }}/evidence/libr232/visit-childrens/)

</aside>

In the Spring of 2013 one of my courses was a survey of Issues in Public
Libraries, and of course one of the greatest issues is measuring success
of a library's services. For one project, in which I visited and
critiqued one service area of a library, I once again found myself
comparing the Hennepin County and Ramsey County libraries, this time in
their Minneapolis and Roseville branches. I specifically compared their
children's departments, and measured both based on criteria and ideals
from several books and articles, cited within the paper. Though I did
not produce a specific list of goals to work toward, there are
nonetheless many measurable criteria by which the two libraries can be
evaluated, btoh against an ideal and in comparison with each other.

I took the criteria by which I judged the libraries from various
suggestions and best practices on how to build and develop a library
children's department. On the subject of the library space itself,
children's departments are recommended to be unique and welcoming,
different from other parts of the library in order to set it apart, and
with decoration and fixtures aimed specifically at their younger (and
shorter) users. Children should be treated as equally important patrons
of the library, with access to the services they need in their own
space. The library collection itself, as a part of the children's
department, should be inclusive of all patrons and all ages, with
different parts of the collection presented however is best for their
target audiences. For specific programs, storytimes are emphasized as
valuable for building understanding of the world, but other interesting
and interactive activities should be included as well.

Based on these criteria, both libraries showed strengths and weaknesses,
though with the Minneapolis library showing itself as far more
successful. Minneapolis had a more unique and specialized library space
for children, and was more inclusive of services and technology within
the department itself. The Roseville library has less variety in the way
it held its children's collection, and though one part of its collection
was superior to Minneapolis, the rest suffered in comparison. Finally,
the Minneapolis library had a much greater variety of available programs
and activities. While some of these differences can be excused by a
difference in budget, with Minneapolis having more available resources
in general, other aspects could be improved based on this evaluation.
For the Roseville library, the failure of certain criteria shows
directly where improvements could be made, such as by changing the way
the children's collection is shelved, or by moving some of the library
computers into the department for children to use.

I took from this assignment that evaluation need not be in a vacuum, and
that it is sometimes just as useful to evaluate a service against a
similar service as to evaluate against an ideal. Comparing the Roseville
library to the Minneapolis library shows clearly how it could be
improved, better than an abstract list of criteria can on its own. The
criteria, though, are still necessary in order to have an unambiguous
yardstick to measure against. Comparing two libraries to each other can
show that one is better. Comparing both to specific and measurable
criteria can show *how* one is better, by how *much*, and what needs to
be done to improve the other. From there, specific actions can be taken,
with the specific goals of improving the service that was shown to fall
short, and there is already a standard in place to measure it against to
see if the actions are successful in improving the service.

Future Application
------------------

This competency is necessary for nearly any part of Library and
Information service. Any action, any service, any program can and should
be evaluated for how effective it is, and if the service offered
justifies the costs in resources. A central part of any such evaluation
is to have a goal that can be reached, and a way to determine if that
goal is being reached. In any part of my career to come, whether in
judging the success of a program, in evaluating the performance of an
employee, or in proposing a new service to be added, I will need to use
what I have learned here about measuring success in order to properly
judge or present the results that have been achieved. If services are to
be judged accurately, they must be measured accurately. If employees are
to be retained, promoted, or let go, it must be based on fair and
quantifiable reasons. No evaluation can be done in a vacuum of
uncertainty, and no evaluation needs to be.

<div class="references">

References
----------

Collins, J. (2005). *Good to great and the social sectors*. New York,
NY: HarperCollins.

Ross, C. S., Nilsen, K., & Radford, M. L. (2009). *Conducting the
reference interview* (2nd ed.). New York, NY: Neal-Schuman Publishers.

Search Institute. (2007). 40 developmental assets for adolescents.
Retrieved from
<http://www.search-institute.org/content/40-developmental-assets-adolescents-ages-12-18>

</div>
